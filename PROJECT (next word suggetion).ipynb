{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"executionInfo":{"elapsed":566,"status":"ok","timestamp":1717997729198,"user":{"displayName":"Deepak singh","userId":"18431869356321379158"},"user_tz":-330},"id":"2ieBtpEFVosy"},"outputs":[],"source":["import numpy as np\n","import pandas as pd"]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":73},"executionInfo":{"elapsed":122644,"status":"ok","timestamp":1717997851827,"user":{"displayName":"Deepak singh","userId":"18431869356321379158"},"user_tz":-330},"id":"E6U0n8szYUHh","outputId":"54713eff-95bf-462b-8073-d5d2d8c3cdf7"},"outputs":[{"data":{"text/html":["\n","     \u003cinput type=\"file\" id=\"files-491d3d8f-555f-4a2c-9524-7d9151e1a44a\" name=\"files[]\" multiple disabled\n","        style=\"border:none\" /\u003e\n","     \u003coutput id=\"result-491d3d8f-555f-4a2c-9524-7d9151e1a44a\"\u003e\n","      Upload widget is only available when the cell has been executed in the\n","      current browser session. Please rerun this cell to enable.\n","      \u003c/output\u003e\n","      \u003cscript\u003e// Copyright 2017 Google LLC\n","//\n","// Licensed under the Apache License, Version 2.0 (the \"License\");\n","// you may not use this file except in compliance with the License.\n","// You may obtain a copy of the License at\n","//\n","//      http://www.apache.org/licenses/LICENSE-2.0\n","//\n","// Unless required by applicable law or agreed to in writing, software\n","// distributed under the License is distributed on an \"AS IS\" BASIS,\n","// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n","// See the License for the specific language governing permissions and\n","// limitations under the License.\n","\n","/**\n"," * @fileoverview Helpers for google.colab Python module.\n"," */\n","(function(scope) {\n","function span(text, styleAttributes = {}) {\n","  const element = document.createElement('span');\n","  element.textContent = text;\n","  for (const key of Object.keys(styleAttributes)) {\n","    element.style[key] = styleAttributes[key];\n","  }\n","  return element;\n","}\n","\n","// Max number of bytes which will be uploaded at a time.\n","const MAX_PAYLOAD_SIZE = 100 * 1024;\n","\n","function _uploadFiles(inputId, outputId) {\n","  const steps = uploadFilesStep(inputId, outputId);\n","  const outputElement = document.getElementById(outputId);\n","  // Cache steps on the outputElement to make it available for the next call\n","  // to uploadFilesContinue from Python.\n","  outputElement.steps = steps;\n","\n","  return _uploadFilesContinue(outputId);\n","}\n","\n","// This is roughly an async generator (not supported in the browser yet),\n","// where there are multiple asynchronous steps and the Python side is going\n","// to poll for completion of each step.\n","// This uses a Promise to block the python side on completion of each step,\n","// then passes the result of the previous step as the input to the next step.\n","function _uploadFilesContinue(outputId) {\n","  const outputElement = document.getElementById(outputId);\n","  const steps = outputElement.steps;\n","\n","  const next = steps.next(outputElement.lastPromiseValue);\n","  return Promise.resolve(next.value.promise).then((value) =\u003e {\n","    // Cache the last promise value to make it available to the next\n","    // step of the generator.\n","    outputElement.lastPromiseValue = value;\n","    return next.value.response;\n","  });\n","}\n","\n","/**\n"," * Generator function which is called between each async step of the upload\n"," * process.\n"," * @param {string} inputId Element ID of the input file picker element.\n"," * @param {string} outputId Element ID of the output display.\n"," * @return {!Iterable\u003c!Object\u003e} Iterable of next steps.\n"," */\n","function* uploadFilesStep(inputId, outputId) {\n","  const inputElement = document.getElementById(inputId);\n","  inputElement.disabled = false;\n","\n","  const outputElement = document.getElementById(outputId);\n","  outputElement.innerHTML = '';\n","\n","  const pickedPromise = new Promise((resolve) =\u003e {\n","    inputElement.addEventListener('change', (e) =\u003e {\n","      resolve(e.target.files);\n","    });\n","  });\n","\n","  const cancel = document.createElement('button');\n","  inputElement.parentElement.appendChild(cancel);\n","  cancel.textContent = 'Cancel upload';\n","  const cancelPromise = new Promise((resolve) =\u003e {\n","    cancel.onclick = () =\u003e {\n","      resolve(null);\n","    };\n","  });\n","\n","  // Wait for the user to pick the files.\n","  const files = yield {\n","    promise: Promise.race([pickedPromise, cancelPromise]),\n","    response: {\n","      action: 'starting',\n","    }\n","  };\n","\n","  cancel.remove();\n","\n","  // Disable the input element since further picks are not allowed.\n","  inputElement.disabled = true;\n","\n","  if (!files) {\n","    return {\n","      response: {\n","        action: 'complete',\n","      }\n","    };\n","  }\n","\n","  for (const file of files) {\n","    const li = document.createElement('li');\n","    li.append(span(file.name, {fontWeight: 'bold'}));\n","    li.append(span(\n","        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n","        `last modified: ${\n","            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n","                                    'n/a'} - `));\n","    const percent = span('0% done');\n","    li.appendChild(percent);\n","\n","    outputElement.appendChild(li);\n","\n","    const fileDataPromise = new Promise((resolve) =\u003e {\n","      const reader = new FileReader();\n","      reader.onload = (e) =\u003e {\n","        resolve(e.target.result);\n","      };\n","      reader.readAsArrayBuffer(file);\n","    });\n","    // Wait for the data to be ready.\n","    let fileData = yield {\n","      promise: fileDataPromise,\n","      response: {\n","        action: 'continue',\n","      }\n","    };\n","\n","    // Use a chunked sending to avoid message size limits. See b/62115660.\n","    let position = 0;\n","    do {\n","      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n","      const chunk = new Uint8Array(fileData, position, length);\n","      position += length;\n","\n","      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n","      yield {\n","        response: {\n","          action: 'append',\n","          file: file.name,\n","          data: base64,\n","        },\n","      };\n","\n","      let percentDone = fileData.byteLength === 0 ?\n","          100 :\n","          Math.round((position / fileData.byteLength) * 100);\n","      percent.textContent = `${percentDone}% done`;\n","\n","    } while (position \u003c fileData.byteLength);\n","  }\n","\n","  // All done.\n","  yield {\n","    response: {\n","      action: 'complete',\n","    }\n","  };\n","}\n","\n","scope.google = scope.google || {};\n","scope.google.colab = scope.google.colab || {};\n","scope.google.colab._files = {\n","  _uploadFiles,\n","  _uploadFilesContinue,\n","};\n","})(self);\n","\u003c/script\u003e "],"text/plain":["\u003cIPython.core.display.HTML object\u003e"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["Saving sherlock-holm.es_stories_plain-text_advs.txt to sherlock-holm.es_stories_plain-text_advs.txt\n"]}],"source":["from google.colab import files\n","path_to_file = list(files.upload().keys())[0]"]},{"cell_type":"code","execution_count":3,"metadata":{"executionInfo":{"elapsed":5,"status":"ok","timestamp":1717997851828,"user":{"displayName":"Deepak singh","userId":"18431869356321379158"},"user_tz":-330},"id":"zDEyFYfbYX9s"},"outputs":[],"source":["# Read the text file\n","with open(path_to_file, 'r', encoding='utf-8') as file:\n","  text = file.read()"]},{"cell_type":"code","execution_count":4,"metadata":{"executionInfo":{"elapsed":3280,"status":"ok","timestamp":1717997855104,"user":{"displayName":"Deepak singh","userId":"18431869356321379158"},"user_tz":-330},"id":"zFoc_rhwYhGF"},"outputs":[],"source":["import tensorflow as tf"]},{"cell_type":"code","execution_count":5,"metadata":{"executionInfo":{"elapsed":9,"status":"ok","timestamp":1717997855105,"user":{"displayName":"Deepak singh","userId":"18431869356321379158"},"user_tz":-330},"id":"N2dCuawZYhn8"},"outputs":[],"source":["from tensorflow.keras.preprocessing.text import Tokenizer\n","from tensorflow.keras.preprocessing.sequence import pad_sequences\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import Embedding, LSTM, Dense"]},{"cell_type":"code","execution_count":6,"metadata":{"executionInfo":{"elapsed":8,"status":"ok","timestamp":1717997855105,"user":{"displayName":"Deepak singh","userId":"18431869356321379158"},"user_tz":-330},"id":"Ra7gYkYWYmWL"},"outputs":[],"source":["#Now let's tokenize the text to create a sequence of words:\n","tokenizer = Tokenizer()\n","tokenizer.fit_on_texts([text])\n","total_words = len(tokenizer.word_index) + 1"]},{"cell_type":"code","execution_count":6,"metadata":{"executionInfo":{"elapsed":7,"status":"ok","timestamp":1717997855105,"user":{"displayName":"Deepak singh","userId":"18431869356321379158"},"user_tz":-330},"id":"CzaSN9J8aMOj"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":7,"metadata":{"executionInfo":{"elapsed":7,"status":"ok","timestamp":1717997855105,"user":{"displayName":"Deepak singh","userId":"18431869356321379158"},"user_tz":-330},"id":"fgTaYv4naOyn"},"outputs":[],"source":["#In the above code, the text is tokenized, which means it is divided into individual words or tokens.\n","#The Tokenizer' object is created, which will handle the tokenization process.\n","#The 'fit_on_texts' method of the tokenizer is called, passing the 'text' as input.\n","#This method analyzes the text and builds a vocabulary of unique words, assigning each word a numerical index.\n","#The 'total_words' variable is then assigned the value of the length of the word index plus one, representing the total number of distinct words in the text."]},{"cell_type":"code","execution_count":7,"metadata":{"executionInfo":{"elapsed":6,"status":"ok","timestamp":1717997855105,"user":{"displayName":"Deepak singh","userId":"18431869356321379158"},"user_tz":-330},"id":"cuGpJHLTapvP"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":8,"metadata":{"executionInfo":{"elapsed":6,"status":"ok","timestamp":1717997855105,"user":{"displayName":"Deepak singh","userId":"18431869356321379158"},"user_tz":-330},"id":"0YEyP6kda0-G"},"outputs":[],"source":["#Now let's create input-output pairs by splitting the text into sequences of tokens and forming n-grams from the sequences:"]},{"cell_type":"code","execution_count":8,"metadata":{"executionInfo":{"elapsed":5,"status":"ok","timestamp":1717997855105,"user":{"displayName":"Deepak singh","userId":"18431869356321379158"},"user_tz":-330},"id":"7UVjyVxJa3b9"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":9,"metadata":{"executionInfo":{"elapsed":761,"status":"ok","timestamp":1717997855861,"user":{"displayName":"Deepak singh","userId":"18431869356321379158"},"user_tz":-330},"id":"IJgz6FBhbY7k"},"outputs":[],"source":["input_sequences = []\n","for line in text.split('\\n'):\n","  token_list = tokenizer.texts_to_sequences([line])[0]\n","  for i in range(1, len(token_list )):\n","    n_gram_sequence = token_list[:i+1]\n","    input_sequences.append(n_gram_sequence)"]},{"cell_type":"code","execution_count":9,"metadata":{"executionInfo":{"elapsed":6,"status":"ok","timestamp":1717997855861,"user":{"displayName":"Deepak singh","userId":"18431869356321379158"},"user_tz":-330},"id":"Xr0WP6Fjbi7h"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":10,"metadata":{"executionInfo":{"elapsed":6,"status":"ok","timestamp":1717997855862,"user":{"displayName":"Deepak singh","userId":"18431869356321379158"},"user_tz":-330},"id":"Njm4VK-Obmcf"},"outputs":[],"source":["# n the above code, the text data is split into lines using the '\\n' character as a delimiter. For each line in the text, the 'texts_to_sequences'\n","# method of the tokenizer is used to convert the line into a sequence of numerical tokens based on the previously created vocabulary.\n","# The resulting token list is then iterated over using a for loop. For each iteration, a subsequence, or n-gram, of tokens is extracted, ranging from\n","# the beginning of the token list up to the current index 'i'."]},{"cell_type":"code","execution_count":11,"metadata":{"executionInfo":{"elapsed":6,"status":"ok","timestamp":1717997855862,"user":{"displayName":"Deepak singh","userId":"18431869356321379158"},"user_tz":-330},"id":"_HB81EG1cwFV"},"outputs":[],"source":["max_sequence_len = max([len(seq) for seq in input_sequences])\n","input_sequences = np.array (pad_sequences (input_sequences, maxlen=max_sequence_len, padding='pre'))"]},{"cell_type":"code","execution_count":12,"metadata":{"executionInfo":{"elapsed":6,"status":"ok","timestamp":1717997855862,"user":{"displayName":"Deepak singh","userId":"18431869356321379158"},"user_tz":-330},"id":"snGncgz2c4av"},"outputs":[],"source":["#In the above code, the input sequences are padded to ensure all sequences have the same length.\n","#The variable 'max_sequence_len' is assigned the maximum length among all the input sequences.\n","#The 'pad_sequences' function is used to pad or truncate the input sequences to match this maximum length.\n","#The 'pad_sequences' function takes the input_sequences list, sets the maximum length to 'max sequence_len',\n","#and specifies that the padding should be added at the beginning of each sequence using the 'padding=pre' argument. Finally,\n","#the input sequences are converted into a numpy array to facilitate further processing.\n","# Now let's split the sequences into input and output:"]},{"cell_type":"code","execution_count":12,"metadata":{"executionInfo":{"elapsed":6,"status":"ok","timestamp":1717997855862,"user":{"displayName":"Deepak singh","userId":"18431869356321379158"},"user_tz":-330},"id":"nAU2_saXdRPT"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":13,"metadata":{"executionInfo":{"elapsed":6,"status":"ok","timestamp":1717997855862,"user":{"displayName":"Deepak singh","userId":"18431869356321379158"},"user_tz":-330},"id":"5DvuEa24eRqY"},"outputs":[],"source":["x = input_sequences[:, :-1]\n","y = input_sequences [:, -1]"]},{"cell_type":"code","execution_count":14,"metadata":{"executionInfo":{"elapsed":5,"status":"ok","timestamp":1717997855862,"user":{"displayName":"Deepak singh","userId":"18431869356321379158"},"user_tz":-330},"id":"tH6RFRPieUno"},"outputs":[],"source":["# In the above code, the input sequences are split into two arrays, 'X' and 'y', to create the input and output for training the next word prediction model. The 'X' array is assigned the values of all rows in the 'input_sequences' array except for the last column. It means that 'X' contains all the tokens in each sequence except for the last one, representing the input context."]},{"cell_type":"code","execution_count":15,"metadata":{"executionInfo":{"elapsed":5,"status":"ok","timestamp":1717997855862,"user":{"displayName":"Deepak singh","userId":"18431869356321379158"},"user_tz":-330},"id":"Q8dHCUgsefMg"},"outputs":[],"source":["# On the other hand, the 'y' array is assigned the values of the last column in the 'input_sequences' array, which represents the target or predicted word."]},{"cell_type":"code","execution_count":16,"metadata":{"executionInfo":{"elapsed":5,"status":"ok","timestamp":1717997855862,"user":{"displayName":"Deepak singh","userId":"18431869356321379158"},"user_tz":-330},"id":"ZxW_tDieegEp"},"outputs":[],"source":["# Now let's convert the output to one-hot encode vectors:"]},{"cell_type":"code","execution_count":17,"metadata":{"executionInfo":{"elapsed":2850,"status":"ok","timestamp":1717997858707,"user":{"displayName":"Deepak singh","userId":"18431869356321379158"},"user_tz":-330},"id":"14GhG1Ijehg5"},"outputs":[],"source":["y= np.array(tf.keras.utils.to_categorical(y, num_classes=total_words))"]},{"cell_type":"code","execution_count":18,"metadata":{"executionInfo":{"elapsed":4,"status":"ok","timestamp":1717997858707,"user":{"displayName":"Deepak singh","userId":"18431869356321379158"},"user_tz":-330},"id":"o5jwav6lemvU"},"outputs":[],"source":["# In the above code, we are converting the output array into a suitable format for training a model, where each target word is represented as a binary vector.\n","# Now let's build a neural network architecture to train the model:"]},{"cell_type":"code","execution_count":19,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":807,"status":"ok","timestamp":1717997859512,"user":{"displayName":"Deepak singh","userId":"18431869356321379158"},"user_tz":-330},"id":"qoQhrJPKex3o","outputId":"4e1b90d6-ea14-4f7f-8536-4749ec0ee8c7"},"outputs":[{"name":"stdout","output_type":"stream","text":["Model: \"sequential\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," embedding (Embedding)       (None, 17, 100)           820000    \n","                                                                 \n"," lstm (LSTM)                 (None, 150)               150600    \n","                                                                 \n"," dense (Dense)               (None, 8200)              1238200   \n","                                                                 \n","=================================================================\n","Total params: 2208800 (8.43 MB)\n","Trainable params: 2208800 (8.43 MB)\n","Non-trainable params: 0 (0.00 Byte)\n","_________________________________________________________________\n","None\n"]}],"source":["model = Sequential()\n","model.add(Embedding (total_words, 100, input_length=max_sequence_len-1))\n","model.add(LSTM(150))\n","model.add(Dense(total_words, activation='softmax'))\n","print(model.summary())"]},{"cell_type":"code","execution_count":19,"metadata":{"executionInfo":{"elapsed":5,"status":"ok","timestamp":1717997859512,"user":{"displayName":"Deepak singh","userId":"18431869356321379158"},"user_tz":-330},"id":"bwRWvX4se1xr"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":20,"metadata":{"executionInfo":{"elapsed":5,"status":"ok","timestamp":1717997859512,"user":{"displayName":"Deepak singh","userId":"18431869356321379158"},"user_tz":-330},"id":"Tp3au_ybgGNv"},"outputs":[],"source":["# The code above defines the model architecture for the next word prediction model. The 'Sequential' model is created, which represents a linear stack of layers.\n","# The first layer added to the model is the 'Embedding' layer, which is responsible for converting the input sequences into dense vectors of fixed size. It takes three arguments:\n","# (1). 'total_words', which represents the total number of distinct words in the vocabulary;\n","# (2). '100', which denotes the dimensionality of the word embeddings;\n","# (3). and 'input length', which specifies the length of the input sequences.\n","# The next layer added is the 'LSTM' layer, a type of recurrent neural network (RNN) layer designed for capturing sequential dependencies in the data.\n","# It has 150 units, which means it will learn 150 internal representations or memory cells.\n","# Finally, the 'Dense' layer is added, which is a fully connected layer that produces the output predictions.\n","# It has 'total_words' units and uses the 'softmax' activation function to convert the predicted scores into probabilities, indicating\n","# the likelihood of each word being the next one in the sequence.\n","# Now let's compile and train the model:"]},{"cell_type":"code","execution_count":20,"metadata":{"executionInfo":{"elapsed":498,"status":"ok","timestamp":1717997870402,"user":{"displayName":"Deepak singh","userId":"18431869356321379158"},"user_tz":-330},"id":"j4PaqftRpLjJ"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true,"base_uri":"https://localhost:8080/"},"id":"84zJPoi-px99"},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch 1/100\n","3010/3010 [==============================] - 187s 62ms/step - loss: 6.2420 - accuracy: 0.0759\n","Epoch 2/100\n","3010/3010 [==============================] - 187s 62ms/step - loss: 5.5222 - accuracy: 0.1236\n","Epoch 3/100\n","3010/3010 [==============================] - 179s 60ms/step - loss: 5.1387 - accuracy: 0.1466\n","Epoch 4/100\n","3010/3010 [==============================] - 189s 63ms/step - loss: 4.8166 - accuracy: 0.1640\n","Epoch 5/100\n","3010/3010 [==============================] - 178s 59ms/step - loss: 4.5145 - accuracy: 0.1820\n","Epoch 6/100\n","3010/3010 [==============================] - 176s 58ms/step - loss: 4.2218 - accuracy: 0.2017\n","Epoch 7/100\n","3010/3010 [==============================] - 183s 61ms/step - loss: 3.9439 - accuracy: 0.2273\n","Epoch 8/100\n","3010/3010 [==============================] - 185s 61ms/step - loss: 3.6805 - accuracy: 0.2578\n","Epoch 9/100\n","3010/3010 [==============================] - 188s 62ms/step - loss: 3.4299 - accuracy: 0.2920\n","Epoch 10/100\n","3010/3010 [==============================] - 193s 64ms/step - loss: 3.1980 - accuracy: 0.3294\n","Epoch 11/100\n","3010/3010 [==============================] - 185s 62ms/step - loss: 2.9835 - accuracy: 0.3647\n","Epoch 12/100\n","3010/3010 [==============================] - 186s 62ms/step - loss: 2.7845 - accuracy: 0.4002\n","Epoch 13/100\n","3010/3010 [==============================] - 185s 61ms/step - loss: 2.5985 - accuracy: 0.4325\n","Epoch 14/100\n","3010/3010 [==============================] - 187s 62ms/step - loss: 2.4303 - accuracy: 0.4668\n","Epoch 15/100\n","3010/3010 [==============================] - 191s 64ms/step - loss: 2.2760 - accuracy: 0.4977\n","Epoch 16/100\n","3010/3010 [==============================] - 188s 62ms/step - loss: 2.1345 - accuracy: 0.5274\n","Epoch 17/100\n","3010/3010 [==============================] - 187s 62ms/step - loss: 2.0053 - accuracy: 0.5526\n","Epoch 18/100\n","3010/3010 [==============================] - 186s 62ms/step - loss: 1.8861 - accuracy: 0.5793\n","Epoch 19/100\n","3010/3010 [==============================] - 190s 63ms/step - loss: 1.7756 - accuracy: 0.6030\n","Epoch 20/100\n","3010/3010 [==============================] - 188s 63ms/step - loss: 1.6772 - accuracy: 0.6231\n","Epoch 21/100\n","3010/3010 [==============================] - 187s 62ms/step - loss: 1.5842 - accuracy: 0.6455\n","Epoch 22/100\n","3010/3010 [==============================] - 187s 62ms/step - loss: 1.5006 - accuracy: 0.6628\n","Epoch 23/100\n","3010/3010 [==============================] - 189s 63ms/step - loss: 1.4248 - accuracy: 0.6796\n","Epoch 24/100\n","3010/3010 [==============================] - 182s 60ms/step - loss: 1.3543 - accuracy: 0.6942\n","Epoch 25/100\n","3010/3010 [==============================] - 179s 59ms/step - loss: 1.2897 - accuracy: 0.7104\n","Epoch 26/100\n","3010/3010 [==============================] - 180s 60ms/step - loss: 1.2291 - accuracy: 0.7218\n","Epoch 27/100\n","3010/3010 [==============================] - 177s 59ms/step - loss: 1.1750 - accuracy: 0.7355\n","Epoch 28/100\n","3010/3010 [==============================] - 178s 59ms/step - loss: 1.1243 - accuracy: 0.7446\n","Epoch 29/100\n","3010/3010 [==============================] - 180s 60ms/step - loss: 1.0833 - accuracy: 0.7544\n","Epoch 30/100\n","3010/3010 [==============================] - 181s 60ms/step - loss: 1.0384 - accuracy: 0.7625\n","Epoch 31/100\n","3010/3010 [==============================] - 179s 59ms/step - loss: 0.9988 - accuracy: 0.7718\n","Epoch 32/100\n","3010/3010 [==============================] - 182s 60ms/step - loss: 0.9633 - accuracy: 0.7804\n","Epoch 33/100\n","3010/3010 [==============================] - 183s 61ms/step - loss: 0.9323 - accuracy: 0.7864\n","Epoch 34/100\n","3010/3010 [==============================] - 185s 62ms/step - loss: 0.9016 - accuracy: 0.7933\n","Epoch 35/100\n","3010/3010 [==============================] - 180s 60ms/step - loss: 0.8756 - accuracy: 0.7988\n","Epoch 36/100\n","3010/3010 [==============================] - 183s 61ms/step - loss: 0.8487 - accuracy: 0.8038\n","Epoch 37/100\n","3010/3010 [==============================] - 182s 61ms/step - loss: 0.8236 - accuracy: 0.8109\n","Epoch 38/100\n","3010/3010 [==============================] - 178s 59ms/step - loss: 0.8043 - accuracy: 0.8143\n","Epoch 39/100\n","3010/3010 [==============================] - 184s 61ms/step - loss: 0.7805 - accuracy: 0.8197\n","Epoch 40/100\n"," 406/3010 [===\u003e..........................] - ETA: 2:39 - loss: 0.6700 - accuracy: 0.8464"]}],"source":["# Now let's compile and train the model:\n","model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n","model.fit(x, y, epochs=100, verbose=1)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"NAKz_gsAqc9d"},"outputs":[],"source":["# In the above code, the model is being compiled and trained. The 'compile' method configures the model for training. The 'loss' parameter is set to 'categorical_crossentropy',\n","# a commonly used loss function for multi-class classification problems. The 'optimizer' parameter is set to 'adam', an optimization algorithm that adapts the learning rate during\n","# training."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"dqLomfcPrMs_"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Zu6av8xvrNlC"},"outputs":[],"source":["# The 'metrics' parameter is set to 'accuracy' to monitor the accuracy during training. After compiling the model, the 'fit' method is called to train the model on\n","# the input sequences 'X' and the corresponding output 'y'. The 'epochs' parameter specifies the number of times the training process will iterate over the entire dataset.\n","# The 'verbose' parameter is set to '1' to display the training process."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jR5XskVHrRFU"},"outputs":[],"source":["# The above code will take more than an hour to execute. Once the code is executed, here's how we can generate the next word predictions using our model:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"XcxB2colrRVn"},"outputs":[],"source":["#seed text = \"I will leave if they\"\n","seed_text = \"Are you\"\n","next_words=3\n","\n","for _ in range(next_words):\n","  token_list = tokenizer.texts_to_sequences([seed_text])[0]\n","  token_list = pad_sequences([token_list], maxlen = max_sequence_len-1, padding='pre')\n","  predicted= np.argmax(model.predict(token_list), axis=-1)\n","  output_word = \"\"\n","  for word, index in tokenizer.word_index.items():\n","    if index == predicted:\n","      output_word= word\n","      break\n","  seed_text += \" \" + output_word\n","print(seed_text)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hX4ZSZXttZ5c"},"outputs":[],"source":[]}],"metadata":{"colab":{"authorship_tag":"ABX9TyNtMpv92TOmyRRWV98mwvot","name":"","version":""},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}